\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newcommand\N[1]{\left\lVert#1\right\rVert}
\begin{document}
\title{Série 5}
\author{David Wiedemann	}
\maketitle
\section*{1}
Dans ce qui suit, $f$ et $g$ seront deux fonctions de $W$, $\lambda$ sera un scalaire réel et $x_0$ un point quelconque de $U$.\\
On notera $Df( x_0) $ et $Dg( x_0) $ les jacobiennes respectives de $f$ et $g$ en $x_{0}$.\\
Finalement, on dénotera par $r_f( x) $ et $r_g( x) $ les restes respectifs de $f$ et $g$, qui satisfont
\[ 
	\lim_{x \to x_0} \frac{r_f( x) }{\N{x-x_0}} = 	\lim_{x \to x_0} \frac{r_g( x) }{\N{x-x_0}} = 0
\]
De manière générale, on supposera donc que $f$ et $g$ s'écrivent sous la forme:
\begin{align*}
	f( x) &= f( x_0) + Df( x_0) ( x-x_0)  + r_f( x) \\
	g( x) &= g( x_0)  + Dg( x_0) ( x-x_0)  + r_g( x) 
\end{align*}
L'existence d'un élément neutre multiplicatif est évidente, on vérifie donc les autres propriétés.
\subsection*{Addition}
Montrons que  $W$ est fermé sous l'opération d'addition
\begin{align*}
	f( x) + g( x) &= f( x_0) + g( x_0) + Df( x_0) ( x-x_0) + Dg( x_0) ( x-x_0) + r_f( x)  + r_g( x) \\
	( f+g) ( x) &= ( f+g) ( x_0)  + ( Df( x_0) + Dg( x_0) ) ( x-x_0)  + ( r_f) ( x) + r_g(x ) 
\end{align*}
Où on a utilisé que l'addition de matrices est linéaire.
Notons que
\[ 
	\lim_{x \to x_0} \frac{r_f( x) + r_g( x) }{\N{x-x_0}} = 0
\]
et ainsi $f+g$ est différentiable en $x_0$, et on en déduit que $f+g$ est différentiable sur $U$ .\\
\subsection*{Multiplication par un Scalaire}
Montrons que $W$ est stable sous la multiplication par un scalaire.
\[ 
	\lambda \cdot f( x) = \lambda f( x_0)  + \lambda \cdot Df( x_0) ( x-x_0)  + \lambda \cdot r_f( x) 
\]
et car
\[ 
	\lim_{x \to x_0} \frac{\lambda r_f( x) }{\N{x-x_0}} = 0
\]
On en déduit que $\lambda \cdot f( x) $ est différentiable en $x_0$ et est donc différentiable sur $U$.\\
\subsection*{Element Neutre additif}

Il est clair que la fonction constante $e( x) = 0$ est différentiable, en effet toutes ses dérivées partielles sont nulles et donc continues, ainsi, par un théorème du cours, $e$ est différentiable et il est clair que
\[ 
	e( x) + f( x) = f( x) = f( x) + e( x) 
\]
pour tout $x \in U$.
\subsection*{Element Opposé}
Pour $f \in W$, la fonction $( -f )$  définie par
\[ 
	( -f) ( x) = - f( x) 
\]
satisfait clairement
\[ 
	( -f) ( x) + f( x) = 0 = e( x)  = f( x) + ( -f) ( x) 
\]
De plus, $-f$ est différentiable car, elle satisfait
\[ 
	( -f) ( x) = ( -f) ( x_0) - Df( x_0) ( x-x_{0}) - r_f( x)
\]


\subsection*{Associativité de l'Addition }
L'associativité  de l'addition sur $W$ suit directement de l'associativité de l'addition sur $ \mathbb{R}$
\subsection*{Associativité de la Multiplication par un Scalaire}
L'associativité de la multiplication par un scalaire suit directement de l'associativité de la multiplication sur $\mathbb{R}$ et du fait que  $W$ est stable par multiplication.\\
En effet, pour $a,b \in \mathbb{R}$, on a clairement
\[ 
	( a  (  b   ( f) ) )= a \cdot ( b\cdot f) = ( ab \cdot f) 
\]
\subsection*{Distributivité}
On a, pour $a,b \in \mathbb{R}$
\[ 
	( a+b) \cdot f( x) = a \cdot f( x) + b \cdot f( x) 
\]

et car, $a \cdot f$ et $b \cdot f$ sont différentiables , leur somme l'est aussi.\\
De même, on a que
\[ 
	a \cdot ( f( x) + g( x) ) = a\cdot f( x) + a\cdot g( x) 
\]
et car $a \cdot f$ et $a\cdot g$ sont différentiables, leur somme l'est aussi.
Ainsi, $W$ est un espace vectoriel.
\section*{2}
Soit $f$ et $g \in W$ comme dans la partie précédente
\begin{align*}
	f( x) &= f( x_0)  + Df( x_0)  ( x-x_0)  + r_f( x) \\
	g( x) &= g( x_0)  + Dg( x_0)  ( x-x_0)  + r_g( x) \\
\end{align*}
Ainsi, on a
\begin{align*}
	f( x) \cdot g( x) &=  f( x_0) g( x_0) + f( x_0) Dg( x_0)( x-x_0)  + f( x_0) r_g( x)\\
				  &+ g( x_0) Df( x_0)( x-x_0) + Df( x_0)( x-x_0)  Dg( x_0) ( x-x_0)+ Df(x_0 )  ( x-x_0) r_g( x)  \\
				  &+ r_f( x) g( x_0) + r_f( x) Dg( x_0) ( x-x_0) + r_f( x) r_g( x) \\
\end{align*}
Il faut donc montrer que
\begin{align*}
	\lim_{\vec{x} \to x_0} \frac{1}{\N { x-x_0} } &\big( Df( x_0)( x-x_0)  Dg( x_0) ( x-x_0)+ Df(x_0 )  ( x-x_0) r_g( x) + f( x_0) r_g( x) \\
&+ r_f( x) g( x_0) + r_f( x) Dg( x_0) ( x-x_0) + r_f( x) r_g( x)\big)\\
\end{align*}
On procède terme par terme, on a
\begin{align*}
\lim_{\vec{x} \to x_0}\quad &\frac{1}{\N { x-x_0} } \vert( Df( x_0)( x-x_0)  Dg( x_0) ( x-x_0) )\vert\\
&\leq \frac{1}{\N { x-x_0} } ( \N { Df( x_0) } \N { x-x_0} \N { Dg( x_0) } \N { x-x_0}  ) =0
\end{align*}
où on a utilisé l'inégalité de Cauchy-Schwarz.\\
De même
\begin{align*}
	\lim_{\vec{x} \to x_0} \frac{r_g( x) }{\N { x-x_0} } ( Df( x_0) ( x-x_0)  + f( x_0)  )  = 0
\end{align*}
et
\begin{align*}
	\lim_{\vec{x} \to x_0} \frac{r_f( x) }{\N { x-x_0} } ( Dg( x_0) ( x-x_0)  + g( x_0) + r_g( x)   )  = 0
\end{align*}
Ainsi, en posant
\begin{align*}
	r_h ( x)  &:= Df( x_0)( x-x_0)  Dg( x_0) ( x-x_0)+ Df(x_0 )  ( x-x_0) r_g( x) + f( x_0) r_g( x) \\
&+ r_f( x) g( x_0) + r_f( x) Dg( x_0) ( x-x_0) + r_f( x) r_g( x)
\end{align*}
On obtient, en utilisant la linéarité de la multiplication matricielle:
\begin{align*}
	( f\cdot g) ( x) &= ( f\cdot g) ( x_0) + f( x_0) Dg( x_0) ( x-x_0)  + g( x_0) Df( x-x_0)  + r_h( x) \\
	( f\cdot g) ( x) &= ( f\cdot g) ( x_0) +\left( f( x_0) Dg( x_0) + g( x_0) Df( x_0) \right) ( x-x_0)  + r_h( x) \\
\end{align*}
Et car, $r_h( x) $ satisfait
\[ 
	\lim_{x \to x_0} \frac{r_h( x) }{\N { x-x_0} } =0
\]
On en déduit que $f \cdot g$ est différentiable et que
\[ 
	D( f\cdot g)( x_0)  =  g(x_0 ) Df( x_0)   + f( x_0)  Dg( x_0) .
\]
Ce qui conclut la démonstration.
















\end{document}
